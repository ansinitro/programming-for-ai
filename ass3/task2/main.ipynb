{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf82f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2edc9",
   "metadata": {},
   "source": [
    "# Step 1. Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5b97a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Character Count</th>\n",
       "      <th>Preview (first 200 chars)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>article1.txt</td>\n",
       "      <td>2814</td>\n",
       "      <td>1. Introduction In recent years, music plagiar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>article2.txt</td>\n",
       "      <td>4450</td>\n",
       "      <td>Introduction Recently, the use of various type...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>article3.txt</td>\n",
       "      <td>4002</td>\n",
       "      <td>Music is an art whose production is very diffi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>article4.txt</td>\n",
       "      <td>3401</td>\n",
       "      <td>Introduction Aided by the internet, plagiarism...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>article5.txt</td>\n",
       "      <td>2009</td>\n",
       "      <td>Rhythm Plagiarism A prominent example for rhyt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Filename  Character Count  \\\n",
       "0  article1.txt             2814   \n",
       "1  article2.txt             4450   \n",
       "2  article3.txt             4002   \n",
       "3  article4.txt             3401   \n",
       "4  article5.txt             2009   \n",
       "\n",
       "                           Preview (first 200 chars)  \n",
       "0  1. Introduction In recent years, music plagiar...  \n",
       "1  Introduction Recently, the use of various type...  \n",
       "2  Music is an art whose production is very diffi...  \n",
       "3  Introduction Aided by the internet, plagiarism...  \n",
       "4  Rhythm Plagiarism A prominent example for rhyt...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = Path('data')\n",
    "\n",
    "text_files = sorted(folder.glob(\"*.txt\"))\n",
    "\n",
    "if not text_files:\n",
    "    raise FileNotFoundError(f\"No .txt files found in folder: {folder.resolve()}\")\n",
    "\n",
    "# Read all documents\n",
    "documents = []\n",
    "characters = []\n",
    "for file in text_files:\n",
    "    text = file.read_text(encoding=\"utf-8\")\n",
    "    documents.append(re.sub(r\"\\s+\", \" \", text).strip())\n",
    "    characters.append(len(text))\n",
    "data = {\n",
    "    \"Filename\": [f.name for f in text_files],\n",
    "    \"Character Count\": characters,\n",
    "    \"Preview (first 200 chars)\": [text[:200].replace(\"\\n\", \" \") for text in documents],\n",
    "}\n",
    "\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f286b14a",
   "metadata": {},
   "source": [
    "# Step 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "953ecc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hacker/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/hacker/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/hacker/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources (only needed once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b38ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- article1.txt ---\n",
      "Total sentences: 29\n",
      "First 3 preprocessed sentences:\n",
      "  ['introduction', 'recent', 'year', 'music', 'plagiarism', 'become', 'serious', 'issue', 'music', 'industry']\n",
      "  ['growing', 'use', 'world', 'wide', 'web', 'revenue', 'loss', 'due', 'plagiarism', 'pirate', 'copy', 'escalating', 'exponentially']\n",
      "  ['korea', 'estimated', 'billion', 'dollar']\n",
      "\n",
      "--- article2.txt ---\n",
      "Total sentences: 36\n",
      "First 3 preprocessed sentences:\n",
      "  ['introduction', 'recently', 'use', 'various', 'type', 'multimedia', 'data', 'image', 'video', 'audio', 'shown', 'explosive', 'growth', 'content', 'based', 'search', 'became', 'great', 'importance']\n",
      "  ['successful', 'content', 'based', 'search', 'indexing', 'scheme', 'query', 'processing', 'scheme', 'key', 'issue', 'considered']\n",
      "  ['despite', 'great', 'advance', 'audio', 'search', 'less', 'investigated', 'either', 'image', 'video', 'search']\n",
      "\n",
      "--- article3.txt ---\n",
      "Total sentences: 33\n",
      "First 3 preprocessed sentences:\n",
      "  ['music', 'art', 'whose', 'production', 'difficult']\n",
      "  ['artist', 'work', 'year', 'produce', 'one', 'song']\n",
      "  ['every', 'song', 'produced', 'artist', 'unique']\n",
      "\n",
      "--- article4.txt ---\n",
      "Total sentences: 29\n",
      "First 3 preprocessed sentences:\n",
      "  ['introduction', 'aided', 'internet', 'plagiarism', 'noticeable', 'globally', 'across', 'author', 'also', 'across', 'language', 'country']\n",
      "  ['alone', 'billion', 'music', 'track', 'sold', 'internationally']\n",
      "  ['number', 'since', 'increased', 'billion']\n",
      "\n",
      "--- article5.txt ---\n",
      "Total sentences: 18\n",
      "First 3 preprocessed sentences:\n",
      "  ['rhythm', 'plagiarism', 'prominent', 'example', 'rhythm', 'plagiarism', 'called', 'amen', 'break']\n",
      "  ['originates', 'funk', 'recording', 'amen', 'brother', 'winstons', 'considered', 'one', 'widely', 'used', 'drum', 'loop', 'history', 'rap', 'electronic', 'music']\n",
      "  ['extraordinary', 'beat', 'pro', 'tected', 'law']\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans a single sentence or small text chunk.\n",
    "    Converts to lowercase, removes punctuation, numbers, and extra spaces.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[[0-9]*\\]', ' ', text)        # remove [1], [2], etc.\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)          # keep only letters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()       # normalize spaces\n",
    "    return text\n",
    "\n",
    "def tokenize_and_lemmatize(text: str):\n",
    "    \"\"\"\n",
    "    Splits text into sentences, tokenizes words,\n",
    "    removes stopwords, and lemmatizes.\n",
    "    Returns: list of lists (each inner list = words in sentence)\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    processed_sentences = []\n",
    "    for sent in sentences:\n",
    "        cleaned = clean_text(sent)\n",
    "\n",
    "        tokens = word_tokenize(cleaned)\n",
    "        tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "\n",
    "        if tokens:  \n",
    "            processed_sentences.append(tokens)\n",
    "\n",
    "    return processed_sentences\n",
    "\n",
    "\n",
    "# === Example usage ===\n",
    "preprocessed_docs = [tokenize_and_lemmatize(doc) for doc in documents]\n",
    "\n",
    "for i, doc in enumerate(preprocessed_docs, start=1):\n",
    "    print(f\"\\n--- article{i}.txt ---\")\n",
    "    print(f\"Total sentences: {len(doc)}\")\n",
    "    print(\"First 3 preprocessed sentences:\")\n",
    "    for sent in doc[:3]:\n",
    "        print(\" \", sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8059e4",
   "metadata": {},
   "source": [
    "# Step 3. Extractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd96f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f87da35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 3 sentences from article1.txt ---\n",
      "  system receives input polyphonic music pcm data output information plagiarized music music title time etc\n",
      "  growing use world wide web revenue loss due plagiarism pirate copy escalating exponentially\n",
      "  respect melody korea entertainment law society conducted survey find people considered music plagiarism\n",
      "\n",
      "--- Top 3 sentences from article2.txt ---\n",
      "  introduction recently use various type multimedia data image video audio shown explosive growth content based search became great importance\n",
      "  music plagiarism detection using melody database three step query processing provides fast search ability taking three step query processing method consists index searching window stitching post processing\n",
      "  unlike previous system plagiarism detection system unique characteristic follows novel similarity model solves problem misjudgment supporting alignment well shifting similarity model\n",
      "\n",
      "--- Top 3 sentences from article3.txt ---\n",
      "  using mfcc entropy mean energy level created robust fingerprinting algorithm enables u differentiate copy original music three feature chosen two major consideration first algorithm extremely efficient meaning pin point plagiarism easily without error\n",
      "  abstract music plagiarism detector alternative approach music genre classification convert audio signal spectrogram extract feature visual representation\n",
      "  abstract music plagiarism detector alternative approach music genre classification convert audio signal spectrogram extract feature visual representation\n",
      "\n",
      "--- Top 3 sentences from article4.txt ---\n",
      "  sacem organization seek protect right original author composer publisher able manually check small percentage registered piece potential copyright violation\n",
      "  development digital medium technology people routinely exposed music everyday environmentscar home restaurant theater shopping mallbut frustrated able learn hear\n",
      "  due fact sampling basically use song song music plagiarism detector national conference discrete mathematics computing ncdmc page related task cover song detection\n",
      "\n",
      "--- Top 3 sentences from article5.txt ---\n",
      "  rhythm plagiarism scarcely covered lit erature closely related rhythm similarity estimation paulus klapuri took melody reference rhythm transformed melody rhythmical string easier compare along structural dimension\n",
      "  originates funk recording amen brother winstons considered one widely used drum loop history rap electronic music\n",
      "  melodic motive considered identical even transposed another key slowed sped interpreted different rhythmic accentuation\n",
      "\n",
      "=== Final 6-sentence Summary ===\n",
      "1. using mfcc entropy mean energy level created robust fingerprinting algorithm enables u differentiate copy original music three feature chosen two major consideration first algorithm extremely efficient meaning pin point plagiarism easily without error\n",
      "2. rhythm plagiarism scarcely covered lit erature closely related rhythm similarity estimation paulus klapuri took melody reference rhythm transformed melody rhythmical string easier compare along structural dimension\n",
      "3. sacem organization seek protect right original author composer publisher able manually check small percentage registered piece potential copyright violation\n",
      "4. introduction recently use various type multimedia data image video audio shown explosive growth content based search became great importance\n",
      "5. development digital medium technology people routinely exposed music everyday environmentscar home restaurant theater shopping mallbut frustrated able learn hear\n",
      "6. music plagiarism detection using melody database three step query processing provides fast search ability taking three step query processing method consists index searching window stitching post processing\n"
     ]
    }
   ],
   "source": [
    "sentences_per_doc = [\n",
    "    [\" \".join(tokens) for tokens in sent_list] \n",
    "    for sent_list in preprocessed_docs\n",
    "]\n",
    "\n",
    "all_sentences = [sent for doc in sentences_per_doc for sent in doc]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(all_sentences)\n",
    "\n",
    "# Compute sentence importance = mean TF-IDF value per sentence\n",
    "sentence_scores = np.array(tfidf_matrix.mean(axis=1)).ravel()\n",
    "\n",
    "# Now split scores back by document\n",
    "split_points = np.cumsum([len(doc) for doc in sentences_per_doc])\n",
    "doc_scores = np.split(sentence_scores, split_points[:-1])\n",
    "\n",
    "# --- Extract 3 top sentences per document ---\n",
    "top_sentences_per_doc = []\n",
    "for i, (sent_list, scores) in enumerate(zip(sentences_per_doc, doc_scores)):\n",
    "    top_idx = np.argsort(scores)[-3:][::-1]  # top 3 by descending score\n",
    "    top_sentences = [sent_list[j] for j in top_idx]\n",
    "    top_sentences_per_doc.append(top_sentences)\n",
    "    print(f\"\\n--- Top 3 sentences from article{i+1}.txt ---\")\n",
    "    for s in top_sentences:\n",
    "        print(\" \", s)\n",
    "\n",
    "# --- Combine all top sentences (6 total) and rank globally ---\n",
    "all_top_sentences = [s for doc in top_sentences_per_doc for s in doc]\n",
    "all_top_matrix = vectorizer.transform(all_top_sentences)\n",
    "all_top_scores = np.array(all_top_matrix.mean(axis=1)).ravel()\n",
    "\n",
    "# Pick 6 overall best sentences\n",
    "top6_idx = np.argsort(all_top_scores)[-6:][::-1]\n",
    "final_summary = [all_top_sentences[i] for i in top6_idx]\n",
    "\n",
    "print(\"\\n=== Final 6-sentence Summary ===\")\n",
    "for i, s in enumerate(final_summary, start=1):\n",
    "    print(f\"{i}. {s}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feae612b",
   "metadata": {},
   "source": [
    "# Step 4. Abstractive summarization (advanced version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcd6ad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hacker/aitu/pfa/ass3/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-24 11:32:41.011184: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Abstractive Summary ===\n",
      "\n",
      "in recent years, music plagiarism has become a serious issue in the music industry . proposed music plagiarism detection system should be a welcome news to the music industry . system receives PCM data as a query and extracts melody from it . calculates melody similarity to the music in the database and retrieves the plagiarized music .\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load model & tokenizer\n",
    "model_name = \"t5-base\"   # alternatives: \"t5-base\" or \"facebook/bart-large-cnn\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Combine all documents into one long text\n",
    "combined_text = \"\\n\".join(documents)\n",
    "\n",
    "# Optional: truncate to avoid token limit (T5-small max â‰ˆ512 tokens)\n",
    "max_input_length = 512\n",
    "inputs = tokenizer(\n",
    "    \"summarize: \" + combined_text,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=max_input_length,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# Generate summary\n",
    "summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=150,        # output length\n",
    "    min_length=60,\n",
    "    length_penalty=2.0,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n=== Abstractive Summary ===\\n\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe080c7",
   "metadata": {},
   "source": [
    "# Step 5. Evaluation of quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e46fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "236ec0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Automatic Evaluation ===\n",
      "\n",
      "ROUGE-1 / ROUGE-2 / ROUGE-L\n",
      "Extractive: {'rouge1': Score(precision=0.986013986013986, recall=0.05276946107784431, fmeasure=0.10017761989342805), 'rouge2': Score(precision=0.5, recall=0.026581804567577687, fmeasure=0.05047991468183435), 'rougeL': Score(precision=0.5594405594405595, recall=0.029940119760479042, fmeasure=0.056838365896980464)}\n",
      "Abstractive: {'rouge1': Score(precision=1.0, recall=0.020209580838323353, fmeasure=0.03961848862802641), 'rouge2': Score(precision=0.9433962264150944, recall=0.018719580681392737, fmeasure=0.03671071953010279), 'rougeL': Score(precision=1.0, recall=0.020209580838323353, fmeasure=0.03961848862802641)}\n",
      "\n",
      "Cosine similarity with original:\n",
      "Extractive: 0.175\n",
      "Abstractive: 0.628\n"
     ]
    }
   ],
   "source": [
    "def evaluate_summary(original_texts, extractive_summary, abstractive_summary):\n",
    "    joined_original = \" \".join(original_texts)\n",
    "\n",
    "    # --- ROUGE scores (abstractive vs original) ---\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_extractive = scorer.score(joined_original, extractive_summary)\n",
    "    rouge_abstractive = scorer.score(joined_original, abstractive_summary)\n",
    "\n",
    "    # --- Cosine similarity using TF-IDF ---\n",
    "    vectorizer = TfidfVectorizer().fit([joined_original, extractive_summary, abstractive_summary])\n",
    "    vectors = vectorizer.transform([joined_original, extractive_summary, abstractive_summary])\n",
    "    cos_sim_matrix = cosine_similarity(vectors)\n",
    "\n",
    "    print(\"=== Automatic Evaluation ===\")\n",
    "    print(\"\\nROUGE-1 / ROUGE-2 / ROUGE-L\")\n",
    "    print(\"Extractive:\", rouge_extractive)\n",
    "    print(\"Abstractive:\", rouge_abstractive)\n",
    "    print(\"\\nCosine similarity with original:\")\n",
    "    print(f\"Extractive: {cos_sim_matrix[0,1]:.3f}\")\n",
    "    print(f\"Abstractive: {cos_sim_matrix[0,2]:.3f}\")\n",
    "\n",
    "evaluate_summary(documents, \" \".join(final_summary), summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
